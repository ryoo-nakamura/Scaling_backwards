

<!DOCTYPE html>
<html>
<head>
	<title>Scaling Backwards: Minimal Synthetic Pre-training?</title>
    <link rel="stylesheet" type="text/css" href="./pvg.css">
    <link rel="shortcut icon" type="image/png" href="./img/cc_logo_1_crop.png">
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <!--<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        .image-container {
            width: 50%;
            float: left;
            text-align: center;
        }
        .equation {
            text-align: center;
            font-style: italic;
        }
        code {
            background-color: transparent; /* 背景色を透明に */
            color: inherit; /* 文字色を通常のテキストと同じに */
            font-family: "Courier New", Courier, monospace;
        }
    </style>
</head>

<body>
<script type="text/javascript" src="./header.js"></script>

<style>
a.myclass {
    color:#DE382D;
    text-decoration: underline
}
 iframe {
    width: 100%;
    height: auto;
  }
    .responsive-iframe {
    position: relative;
    padding-bottom: 56.25%; /* 16:9のアスペクト比 */
    height: 0;
    overflow: hidden;
  }

  .responsive-iframe iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
  }
</style>

<style>
a.link {
    text-decoration: underline
}
</style>


<h1 align="center" style="font-size: 30pt;"><b>Scaling Backwards: Minimal Synthetic Pre-training?</b></h1><br/>
<center>
    <font color="#c7254e"><b>Accepted by European Conference on Computer Vision 2024 (ECCV) </b></font><br><br>
    <a href="https://ryoo-portfolio.netlify.app" class="">Ryo Nakamura</a><sup>1,*</sup> &emsp; 
    Ryu Tadokoro<sup>2,*</sup> &emsp; 
    <a href="https://www.ryosuke-yamada.net/" class="">Ryosuke Yamada</a><sup>1</sup> &emsp; 
    <a href="https://yukimasano.github.io/" class="">Yuki M. Asano</a><sup>3</sup> &emsp; 
    <a href="https://scholar.google.de/citations?user=n9nXAPcAAAAJ&hl=en" class="">Iro Laina</a><sup>4</sup> &emsp; 
    <a href="https://chrirupp.github.io/" class="">Christian Rupprecht</a><sup>4</sup> &emsp; 
    <a href="https://mmai.tech/" class="">Nakamasa Inoue</a><sup>4</sup> &emsp; 
    <a href="https://www.rio.gsic.titech.ac.jp/jp/member/yokota.html" class="">Rio Yokota</a><sup>1,2</sup> &emsp; 
    <a href="http://hirokatsukataoka.net/" class="">Hirokatsu Kataoka</a><sup>1</sup> &emsp; 
    <br>
    1: AIST &emsp; 2: Tohoku University &emsp; 3: University of Technology Nuremberg <br> &emsp; 4: University of Oxford  &emsp; 5: Tokyo Institute of Technology<br> 
    &emsp; *: These authors contributed equally<br><br>
    
    <section class="delta">
        <div class="container">
            <a href="https://arxiv.org/pdf/2408.00677v1"><button class="btn btn-gray">Paper</button></a>
            <a href="https://github.com/super-tadory/1p-frac"><button class="btn btn-gray">Code</button></a>
            <a href="https://drive.google.com/drive/folders/1iJSiDaqsoWWP5s7Z5BvI1XypXSOPXV1R"><button class="btn btn-gray">Dataset</button></a>
            <!-- <a href="https://drive.google.com/file/d/1x7eV8jvZOrpdrQFP4tgfiH7HS9wIg53d/view"><button class="btn btn-gray">Supp</button></a> -->
            <!-- <a href="./material/ICCV2023_poster.pdf"><button class="btn btn-gray">Poster</button></a> -->
            <br>
            <link rel="shortcut icon" type="image/png" href="./img/cc_logo_1_crop.png">
            <a href="https://hirokatsukataoka16.github.io/Pretraining-without-Natural-Images/"><button class="btn btn-gray">Related Work 1 (IJCV22)<br>FDSL Proposal</button></a>
            <a href="https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/"><button class="btn btn-gray">Related Work 2 (AAAI22)<br>ViT Pre-training</button></a>
            <a href="https://ryoo-nakamura.github.io/One-instance-FractalDB/"><button class="btn btn-gray">Related Work 3 (ICCV23)<br>2D/3D-OFDB</button></a>

        </div>
    </section>
<!--     <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Kataoka_Pre-training_without_Natural_Images_ACCV_2020_paper.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Paper</a> -->
<!--     <a href="https://github.com/hirokatsukataoka16/FractalDB" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Code</a> -->
    <!-- <a href="#dataset" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Dataset</a> -->
<!--     <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_oral.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Oral</a> -->
<!--     <a href="./material/ICCV2023_poster.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Poster</a> -->
<!--     <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_fractaldb_supplementary.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Supp. Mat.</a> -->
    <!-- <a href="https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Related Work</a> -->
    <br><br>
    
   <!-- <div class="responsive-iframe">
  <iframe width="1120" height="630" src="https://www.youtube.com/embed/BzgNBwZt1W4?si=cMYiWL1L06md-V6m" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe> -->
</div>


</center>

<!-- <center>
        <img src="./img/ICCV2023_poster.pdf" style="width: 100%;"/>
</center> -->

<br>
<h2>Abstract</h2>
<p>
    Pre-training and transfer learning are an important building block of current computer vision systems. While pre-training is usually performed on large real-world image datasets, in this paper we ask
    whether this is truly necessary. To this end, we search for a minimal,
    purely synthetic pre-training dataset that allows us to achieve performance similar to the 1 million images of ImageNet-1k. We construct
    such a dataset from a single fractal with perturbations. With this, we
    contribute three main findings. (i) We show that pre-training is effective
    even with minimal synthetic images, with performance on par with largescale pre-training datasets like ImageNet-1k for full fine-tuning. (ii) We
    investigate the single parameter with which we construct artificial categories for our dataset. We find that while the shape differences can be
    indistinguishable to humans, they are crucial for obtaining strong performances. (iii) Finally, we investigate the minimal requirements for successful pre-training. Surprisingly, we find that a substantial reduction of
    synthetic images from 1k to 1 can even lead to an increase in pre-training
    performance, a motivation to further investigate “scaling backwards”. Finally, we extend our method from synthetic images to real images to
    see if a single real image can show similar pre-training effect through
    shape augmentation. We find that the use of grayscale images and affine
    transformations allows even real images to “scale backwards”. The code
    is available at <a href="https://github.com/SUPER-TADORY/1p-frac" class="">link</a>
    .
</p>

<!-- <center>
        <img src="./img/main_image.png" style="width: 97%;"/>
</center> -->
<figure style="text-align: center;">
    <!-- <figcaption>Comparison of ImageNet-1k, FractalDB and 1p-frac (ours). 1p-frac consists
        of only a single fractal for pre-training. With 1p-frac, neural networks learn to classify perturbations applied to the fractal. In our study “single” means a very narrow
        distribution over parameters that leads to images that are roughly equivalent from
        a human visual perspective. While the shape differences of perturbed images can be
        indistinguishable to humans, models pre-trained on 1p-frac achieve comparable performance with those pre-trained on ImageNet-1k or FractalDB.</figcaption> -->
    <img src="./img/main_image.png" style="width: 100%;" alt="image2"/>
</figure>


<!-- <br>
<h2>Why fractal pre-training with one-instance per category can train visual representation?</h2>
We consider instance augmentation of FractalDB. In FractalDB, the number of images is increased by (i) image rotation (x4), (ii) IFS parameters fluctuation (x25), and (iii) patch patterns (x10) from a representative image of the category found with category search. FractalDB is pre-trained as a dataset of 1,000 instances of pre-processed images, but in a simple view, these could be replaced by data augmentation. Details will be discussed later, and Table 6 shows that the accuracy is almost the same with and without Rotation (2D-OFDB w/o rotation 84.0 vs. w/ rotation 84.1 on CIFAR-100). On the other hand, the performance rates with IFS fluctuation are significantly lower (2D-OFDB w/o IFS 84.0 vs. w/ IFS 81.6 on CIFAR-100). From these observations, we believe that a pre-training dataset with fewer instances like a dataset consists of one-instance per category and using basic data augmentation are the key technologies to reduce training time to an equivalent or better accuracy level. As the result, an image augmentation with patch patterns were newly implemented as random patch/texture augmentation so that they could be implemented within data augmentation, and it became clear that accuracy could be improved while reducing the data size to 0.1% amount.

<br><br>
<figure style="text-align: center;">
    <img src="./img/framework.png" style="width: 70%;" alt="image0"/>
    <figcaption>Table 1: Effects of data augmentation for fractal images. <br>Data-efficient image Transformer (DeiT) augmentation set- ting is used as a default.</figcaption>
</figure> -->

<h3>Pre-training with a Single Fractal (1p-frac)</h3>
<!-- <br><br> -->
<figure style="text-align: center;">
    <figcaption>Fig. 2: Scaling backwards from many images to a single synthetic image. (a) Empirical
        distribution <span>\( p_{\text{data}} \)</span>. Colors indicate classes. With a single image, the distribution is
        given by a single Dirac’s delta function. (b) LIEP distribution <span>\( p_{\Delta} \)</span> The support of
        the distribution narrows as the degree of perturbation ∆ decreases. (c) σ-factor for
        investigating fractal shapes. A small σ produces complex fractals.</figcaption>
    <img src="./img/fig2.png" style="width: 100%;" alt="image2"/>
</figure>
We propose 1p-frac to explore a minimally composed pre-training dataset. (The image examples are shown in Fig.2.) 1p-frac is a dataset consisting of fractal images generated from iterated function systems (IFS), and it reduces the number of fractals in order compared to conventional datasets such as FractalDB and One-instance FractalDB (OFDB).

FractalDB is a dataset composed of 1M images and 1k categories, while OFDB consists of 1k images and 1k categories. The empirical distribution of these datasets is shown in Figure (a).

On the other hand, 1p-frac consists of a single fractal and its perturbations, which are learned using the Locally Perturbed Cross Entropy (LPCE) loss. To control a single fractal in this case, we introduce the σ-factor and perform analysis on minimal pre-training.

For more details, please refer to the main text.




<h2>Can 1p-frac provide a pre-training effect comparable to conventional datasets?</h2>
<b>Pre-training with 1p-frac.</b> 
Table 2 compares 1p-frac with FracdalDB (1M
images, 1k categories) and OFDB (1k images, 1k categories). As can be seen, the
pre-training effect of 1p-frac is comparable to that of FractalDB and OFDB.
The number of fractal categories is significantly reduced from C = 1,000 to only
1. 
<br><br>
<figure style="text-align: center;">
    <figcaption>Table 2: Comparison of
        1p-frac with FractalDB
        and OFDB. ⋄
        indicates
        the use of LPCE loss. We report top-1 accuracies (%).</figcaption>
    <img src="./img/table2.png" style="width: 50%;" alt="image2"/>
</figure>

<b>ImageNet-1k fine-tuning. </b> 
While this work discusses minimal requirements for visual pre-training, increasing the approximation precision of the
numerical integration could further improve the performance and benefit training of large models. In Table 8, we applied the LPCE loss with L = 21,000 to the
ViT-B model and compare fine-tuning accuracy on ImageNet-1k. Surprisingly,
and despite using a single fractal as data, our ViT-B pre-trained on 1p-frac
outperforms pre-training on ImageNet-21k. This shows the strength of our approach in particular because we simply keep the same fine-tuning protocols as
used for the original Imagenet-21k to ImageNet-1k transfer,
putting us at a potential disadvantage.
<br><br>
<figure style="text-align: center;">
    <figcaption>Table 8: Comparison of pre-training datasets
        on ImageNet-1k fine-tuning. Fine-tuning accuracies
        calculated by using ViT-Base (B) are listed in the
        table. 21k indicates the number of categories in each
        pre-training dataset.
       </figcaption>
    <img src="./img/table8.png" style="width: 60%;" alt="image1"/>
</figure>




<h2>Experiments on why pre-training works with 1p-frac</h2>
<b>Experiment on perturbation degree.</b> 
Table 5 shows the results obtained by different perturbation degrees ∆ for the LIEP distribution. The results indicate that setting the value to 0.1 yields the highest performance. Interestingly, the performance significantly improved when ∆ was increased from 0.01 to 0.05. 
This suggests that the support of the empirical distribution must have a certain size to obtain positive pre-training effects.

<br><br>
<div class="image-container">
    <figure style="text-align: center;">
        <figcaption>Table 5: Effects of perturbation degree ∆ (σ = 3.5).
            </figcaption>
        <img src="./img/table5.png" style="width: 70%;" alt="image1"/>
    </figure>
</div>
<div class="image-container">
    <figure style="text-align: center;">
        <figcaption>These images are perturbation degree ∆ samples.</figcaption>
        <img src="./img/delta.png" style="width: 80%;" alt="image1"/>
    </figure>
</div>

<br><br>
<b>Experiment on σ-factor.</b> 
Table 6 investigates the effects of the σ-factor, which measures the
complexity of fractal shapes. As can be seen, the most complex shape yields the
highest performance. It is worth noting that, even with a fractal of σ = 6.0,
which looks like Gaussian noise, we observed positive pre-training effects. This
suggests that the shapes obtained from IFS are crucial for pre-training.
<!-- <br><br>
<figure style="text-align: center;">
    <figcaption>Table 6: Effects of σ.
        (∆ = 0.1).</figcaption>
    <img src="./img/table6.png" style="width: 50%;" alt="image1"/>
</figure> -->

<br><br>
<div class="image-container">
    <figure style="text-align: center;">
        <figcaption>Table 6: Effects of σ.
            </figcaption>
        <img src="./img/table6.png" style="width: 80%;" alt="image1"/>
    </figure>
</div>
<div class="image-container">
    <figure style="text-align: center;">
        <figcaption>These images are perturbation degree ∆ samples.</figcaption>
        <img src="./img/sigma.png" style="width: 75%;" alt="image1"/>
    </figure>
</div>


<br><br><br>
<br>

<b>Experiment on numerical integration. </b> 
Tables 7 reduces the number of sampling points L
of the numerical integration when approximately computing loss.
We see that larger numbers result in high performance. Interestingly, even with
L = 16, the pre-training effect was positive (better than training from scratch).
<br><br>
<figure style="text-align: center;">
    <figcaption>Table 7: Effects of number of sampling points L
        for numerical integration.
        </figcaption>
    <img src="./img/table7.png" style="width: 40%;" alt="image1"/>
</figure>

<!-- <h2>Analysis and Discussion</h2> -->

<b>Pre-training with a single real image and shape augmentation. </b> 
It was found that pre-training effects comparable to
a dataset of one million realworld images can be achieved
with 1p-frac, but is it possible to perform pre-training
similarly with a real image? 
In this section, we verify whether visual features can be acquired through pre-training using a single real image (each image in Figure 5) with the LPCE loss using image representations similar to FDSL (Canny images) and geometric transformations for perturbation
({affine, elastic, polynomial} transformations in Figure 4). 
<br><br>
<figure style="text-align: center;">
    <figcaption>Fig. 4: Shape augmentation with three geometry
        transformations from a single real image.</figcaption>
    <img src="./img/fig4.png" style="width: 70%;" alt="image1"/>
</figure>
<figure style="text-align: center;">
    <figcaption>Fig. 5: The five different images used in the pre-training with a single real image and
        shape augmentation.</figcaption>
    <img src="./img/fig5.png" style="width: 90%;" alt="image1"/>
</figure>
The results of pretraining effects with L = 1, 000 are shown in Tables 11 and 12. 
Note that the results in these tables represent the average score on C100 over the five real
images in Figure 5. See the supplementary material for results for each of image. According to the results, we find that contour-emphasized Canny images
omitting color information and affine transformation perturbation yield better
pre-training, as measured by the fine-tuning accuracies. 
The operations are similar to the classification of labeled fractal images via IFS.

<br><br>
<div class="image-container">
    <figure style="text-align: center;">
        <figcaption>Table 11: Comparison of pre-training
            effect between RGB and Canny images
            when shape augmentation is applied. The
            shape augmentation uses affine transformations.</figcaption>
        <img src="./img/table11.png" style="width: 90%;" alt="image1"/>
    </figure>
</div>
<div class="image-container">
    <figure style="text-align: center;">
        <figcaption>Table 12: Comparison of pre-training effects of affine, elastic, and polynomial image transformations. Real Img type with
            Canny applied to the images.</figcaption>
        <img src="./img/table12.png" style="width: 90%;" alt="image1"/>
    </figure>
</div>
<br><br><br>

<b>Linear probing. </b> 
To compare supervised ImageNet-1k pre-training
with our 1p-frac pre-training in the context of early layer visual representations,
we executed a linear probing experiment on the C100 dataset. In the ViT-T
model with {1, 2, 3} layers, the 1p-frac pre-trained model outperformed the
model pre-trained on ImageNet-1k. This demonstrates the superior quality of
early, low-level representations learned from our dataset compared to ImageNet.
In effect, layers up to layer 3 in ViT-T can be sufficiently pre-trained and frozen
using our 1p-frac.
<br><br>
<figure style="text-align: center;">
    <figcaption>Fig. 3: Linear probing following the experiments. We verified the performance
        with {1, 2, 3, 4, 6} layers in the ViT-Tiny model
        on C100 dataset. Here, 2 and 4 show the use of
        up to 1–2 and 1–4 layers.</figcaption>
    <img src="./img/fig3.png" style="width: 70%;" alt="image1"/>
</figure>

<h2>Citation</h2>
<!-- @article{KataokaIJCV2022,<br>
&emsp;author     = {Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},<br>
&emsp;title      = {Pre-training without Natural Images},<br>
&emsp;journal    = {International Journal of Computer Vision (IJCV)},<br>
&emsp;year       = {2022}<br>
} -->
<!-- <br><br> -->
@inproceedings{Nakamura_2024_ECCV,<br>
&emsp;author     = {{Ryo Nakamura and Ryu Tadokoro and Ryosuke Yamada and Yuki M. Asano and Iro Laina and Christian Rupprecht and Nakamasa Inoue and Rio Yokota and Hirokatsu Kataoka},<br>
&emsp;title      = {Scaling Backwards:Minimal Synthetic Pre-training?},<br>
&emsp;booktitle    = {Proceedings of the 2024 European Conference on Computer Vision (ECCV)},<br>
&emsp;month    = {October},<br>
&emsp;year       = {2024}<br>
<!-- &emsp;pages       = {20360-20369}<br> -->
}
<br><br>

<a name="dataset"><h2>Dataset Download</h2></a>
<!-- <ul>
    <li>
        FractalDB-1k (1k categories x 1k instances; Total 1M images).
        <a href="https://drive.google.com/file/d/1KKqz0H7i_TXFMa2oJtcfry9bmAxyS_SS/view?usp=sharing">[Dataset (13GB)]</a>
    </li>
    <li>
        FractalDB-60 (60 well-known categories x 1k instances; Total 60k images).
        <a href="https://drive.google.com/file/d/1F0aEogTScpABjJhNZJaCFT-J8mkdP86o/view?usp=sharing">[Dataset (1.2GB)]</a>
    </li>
</ul>
<br><br> -->

<div style="display: flex;">
    <!-- 列1 -->
    <div style="flex: 1; text-align: center;">
        <img src="img/1p-frac.png" alt="画像1" width="98%"/>
        <!-- テキストリンク1 -->
        <p>Download link :
            <a href="https://drive.google.com/file/d/1tVsOJlju5ATXj8GT0Qt9TzdBopuxGm6D/view" class=""> [1p-frac]</a>
    
        </p>
    </div>
</div>


<h2>Acknowledgement</h2></a>
<ul>
    <li> This work is based on results obtained from a project, JPNP20006, commissioned by the New Energy and Industrial Technology Development Organization (NEDO).</li>
    <li> This work was supported by JSPS KAKENHI Grant Number JP19H01134.</li>
    <li> Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used.</li>
</ul>

<br><br><br>
<script type="text/javascript" src="./footer.js"></script>
</body></html>
